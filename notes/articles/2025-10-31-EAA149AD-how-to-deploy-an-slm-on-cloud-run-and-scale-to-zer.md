---
title: How to Deploy an SLM on Cloud Run and Scale to Zero
url: https://medium.com/@mail.fede.cesarini/how-to-deploy-an-slm-on-cloud-run-and-scale-to-zero-a381c3ee8794
tags: [tutorial, code, news, article, development, ai, machine-learning, documentation]
category: Article
date: 2025-10-31
id: EAA149AD-9E96-4E6C-9FD5-D64AA6464737
created: 2025-10-31T15:05:44Z
modified: 2025-11-01T00:25:44Z
---
# How to Deploy an SLM on Cloud Run and Scale to Zero

## Summary

In a time in which privacy of data becomes more and more important, having your self-hosted language model becomes a must.
There are many…

**Category:** `Article`  

**Tags:** `tutorial` `code` `news` `article` `development` `ai` `machine-learning` `documentation`

**Source:** [https://medium.com/@mail.fede.cesarini/how-to-deploy-an-slm-on-cloud-run-and-scale-to-zero-a381c3ee8794](https://medium.com/@mail.fede.cesarini/how-to-deploy-an-slm-on-cloud-run-and-scale-to-zero-a381c3ee8794)

---

## Content

Press enter or click to view image in full size How to Deploy an SLM on Cloud Run and Scale to Zero Federico Cesarini 4 min read · Oct 16, 2025 -- Listen Share In a time in which privacy of data becomes more and more important, having your self-hosted language model becomes a must. There are many open-weight language models that can perform almost like the big ones, but the reason why few companies are self-hosting them is because it’s often not worth it. The hosting costs are often higher than relying on the API of a colossus like OpenAI or Anthropic. This brings us to a possible solution: hosting SLMs, models that, in some tasks, can perform almost as well as LLMs while using a fraction of the computational power. This guide shows how to self-host your own SLM using Google Cloud Run , Docker , and Ollama , allowing you to scale to zero when no resources are needed. 1. Authenticate to Google Cloud If you haven’t already done it, authenticate to GCP using gcloud (or gsutil if needed). Based on your operating system, you can find the guide here: https://cloud.google.com/storage/docs/gsutil_install Then set the GCP project you are working on: gcloud config set project <your-project-id> Enable Cloud Run and Artifact Registry on the current project: gcloud services enable run.googleapis.com gcloud services enable artifactregistry.googleapis.com Set the default region (this guide uses europe-west4 ). Pay attention that the region must support the GPU of your choice: gcloud config set artifacts/location europe-west4 gcloud config set run/region europe-west4 Create the Artifact Registry repository: gcloud artifacts repositories create hosted-slms --repository-format=docker --location=europe-west4 --description="Hosted SLM images" Check available GPU regions: https://cloud.google.com/compute/docs/gpus/gpu-regions-zones 2. Choose your Ollama Model You can use any model available in the Ollama library . This guide is generic and can be applied to any model. Example model page: https://ollama.com/library/qwen3-coder:30b-a3b-q4_K_M 3. Choose the Hardware For lightweight SLMs, an NVIDIA L4 GPU and 32 GB of RAM are usually sufficient. These specs will be used for your Cloud Run service. 4. Create your Dockerfile We’ll use the latest Ollama Docker image. You can copy-paste this file and change the MODEL environment variable to match your model. FROM ollama/ollama:latest ENV OLLAMA_HOST=0.0.0.0:8080 ENV OLLAMA_MODELS=/models ENV OLLAMA_ORIGINS=* ENV OLLAMA_DEBUG=false ENV OLLAMA_KEEP_ALIVE=-1 ENV MODEL="your-model-name" RUN ollama serve & sleep 5 && ollama pull $MODEL ENTRYPOINT ["ollama", "serve"] 5. Build and Push Your Docker Image Move into the folder where your Dockerfile is located. Export your image variable: export IMAGE="europe-west4-docker.pkg.dev/<your-project-id>/hosted-slms/<your-image-name>" Build and push: gcloud builds submit --tag $IMAGE . 6. Deploy on Cloud Run Deploy your image to Cloud Run: gcloud run deploy slm-service --image $IMAGE --region europe-west4 --memory 32Gi --cpu 8 --concurrency 4 --set-env-vars OLLAMA_NUM_PARALLEL=4 --gpu 1 --gpu-type nvidia-l4 --no-cpu-throttling --no-gpu-zonal-redundancy --max-instances 1 --timeout 600 --allow-unauthenticated ⚠️ The service is unauthenticated for testing purposes, do not deploy like this in production! When idle, Cloud Run will scale to zero , meaning no cost when unused. Cold Start Behavior When Cloud Run scales to zero (no active instances), the first request triggers a cold start . This means Google Cloud must start a new container, load the model, and initialize the runtime. Cold starts are what make scale to zero possible, ensuring you pay nothing when the service is idle. The tradeoff is a short delay on the first request, usually between 10 and 40 seconds , depending on image size and model loading time. To reduce this delay, you can: Keep one instance always warm using --min-instances=1 Use Cloud Scheduler to ping the endpoint periodically Optimize your image and startup logic 7. Test the Service You can use curl or similar tools to test your model endpoint: curl -X POST https://<your-cloud-run-url>/v1/chat/completions -H "Content-Type: application/json" -d &#x27;{ "model": "your-model-name", "messages": [ {"role": "user", "content": "Write a Python function that checks if a number is prime."} ] }&#x27; 8. Integration with Coding Agents This step is critical when using your hosted model for coding tasks. Several agent frameworks can connect to your Cloud Run endpoint if it exposes an OpenAI-compatible API . - Qwen Code https://github.com/QwenLM/qwen-code - OpenCode https://github.com/opencode-ai/opencode Example ~/.config/opencode/opencode.json : { "$schema": "https://opencode.ai/config.json", "provider": { "mycloudrun": { "npm": "@ai-sdk/openai-compatible", "name": "MyCloudRun", "options": { "baseURL": "https://<your-cloud-run-url>/v1" }, "models": { "your-model-name": { "ninteractionsame": "My SLM" } } } }, "model": "mycloudrun/My SLM" } - Claude Code Router https://github.com/musistudio/claude-code-router Example .claude-code-router/config.json : { "Providers": [ { "name": "my-slm", "api_base_url": "https://<your-cloud-run-url>/v1/chat/completions", "api_key": "aaa", "models": ["your-model-name"] } ], "Router": { "default": "my-slm,your-model-name" } } - OpenHands https://github.com/All-Hands-AI/OpenHands This project provides tools for local code execution and AI-assisted development. Conclusion The models perform well in code generation tasks. With the specs above, the cost is around €8 for 8 hours of continuous runtime. Tool calling is still experimental, so agents like Claude Code or OpenCode may have partial support, though this is improving rapidly. Self-hosting an SLM allows full control over data privacy and flexibility, while Cloud Run’s scale-to-zero ensures you only pay when you actually use it.
