---
title: "Glyph: Scaling Context Windows via Visual-Text Compression"
url: https://arxiv.org/abs/2510.17800
tags: []
category: Article
date: 2025-11-01
id: C4C9C8AB-F88E-4383-A81F-18C86ADA1A3F
created: 2025-11-01T10:46:01Z
modified: 2025-11-01T10:46:01Z
---
# Glyph: Scaling Context Windows via Visual-Text Compression

## Summary

Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the p

**Category:** `Article`  

**Source:** [https://arxiv.org/abs/2510.17800](https://arxiv.org/abs/2510.17800)

---

## Content

Glyph: Scaling Context Windows via Visual-Text Compression

[2510.17800] Glyph: Scaling Context Windows via Visual-Text Compression Happy Open Access Week from arXiv! YOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all. We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. > cs > arXiv:2510.17800 Help | Advanced Search All fields Journal reference ACM classification MSC classification Report number arXiv identifier arXiv author ID Help pages quick links Help Pages Computer Science > Computer Vision and Pattern Recognition arXiv:2510.17800 (cs) [Submitted on 20 Oct 2025 ( v1 ), last revised 21 Oct 2025 (this version, v2)] Title: Glyph: Scaling Context Windows via Visual-Text Compression Authors: Jiale Cheng , Yusen Liu , Xinyu Zhang , Yulin Fei , Wenyi Hong , Ruiliang Lyu , Weihan Wang , Zhe Su , Xiaotao Gu , Xiao Liu , Yushi Bai , Jie Tang , Hongning Wang , Minlie Huang View a PDF of the paper titled Glyph: Scaling Context Windows via Visual-Text Compression, by Jiale Cheng and 13 other authors HTML (experimental) Abstract: Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at this https URL . Computer Vision and Pattern Recognition (cs.CV) ; Computation and Language (cs.CL); Machine Learning (cs.LG) arXiv:2510.17800 [cs.CV] arXiv:2510.17800v2 [cs.CV] for this version) https://doi.org/10.48550/arXiv.2510.17800 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jiale Cheng [ view email ] [v1] Mon, 20 Oct 2025 17:58:56 UTC (3,956 KB) Tue, 21 Oct 2025 17:12:48 UTC (4,113 KB) Full-text links: Access Paper: View a PDF of the paper titled Glyph: Scaling Context Windows via Visual-Text Compression, by Jiale Cheng and 13 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.CV | 2025-10 Change to browse by: References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation BibTeX formatted citation Data provided by: Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Institution About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) Privacy Policy Web Accessibility Assistance arXiv Operational Status
