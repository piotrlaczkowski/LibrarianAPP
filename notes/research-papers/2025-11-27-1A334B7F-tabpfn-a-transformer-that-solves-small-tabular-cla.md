---
title: "TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second"
url: https://share.google/hVQT7n5Xoi4zPvJ2Y
tags: []
category: Research Paper
date: 2025-11-27
id: 1A334B7F-47D5-4B7F-88C5-57ED0B8C2E5E
created: 2025-11-27T16:20:59Z
modified: 2025-11-27T16:20:59Z
---
# TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second

## Summary

We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN performs in-context learning (ICL), it learns to

**Category:** `Research Paper`  

**Source:** [https://share.google/hVQT7n5Xoi4zPvJ2Y](https://share.google/hVQT7n5Xoi4zPvJ2Y)

---

## Content

[2207.01848] TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. > cs > arXiv:2207.01848 Help | Advanced Search All fields Journal reference ACM classification MSC classification Report number arXiv identifier arXiv author ID Help pages quick links Help Pages Computer Science > Machine Learning arXiv:2207.01848 (cs) [Submitted on 5 Jul 2022 ( v1 ), last revised 16 Sep 2023 (this version, v6)] Title: TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second Authors: Noah Hollmann , Samuel MÃ¼ller , Katharina Eggensperger , Frank Hutter View a PDF of the paper titled TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second, by Noah Hollmann and 3 other authors Abstract: We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN performs in-context learning (ICL), it learns to make predictions using sequences of labeled examples (x, f(x)) given in the input, without requiring further parameter updates. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to 230$\times$ speedup. This increases to a 5 700$\times$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML. We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at this https URL . Machine Learning (cs.LG) ; Machine Learning (stat.ML) arXiv:2207.01848 [cs.LG] arXiv:2207.01848v6 [cs.LG] for this version) https://doi.org/10.48550/arXiv.2207.01848 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Noah Hollmann [ view email ] [v1] Tue, 5 Jul 2022 07:17:43 UTC (1,473 KB) Sat, 1 Oct 2022 08:53:17 UTC (2,132 KB) Wed, 12 Oct 2022 07:08:27 UTC (2,132 KB) Tue, 29 Nov 2022 18:31:44 UTC (2,228 KB) Sun, 7 May 2023 19:43:41 UTC (4,339 KB) Sat, 16 Sep 2023 09:33:32 UTC (4,340 KB) Full-text links: Access Paper: View a PDF of the paper titled TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second, by Noah Hollmann and 3 other authors View PDF TeX Source view license Current browse context: cs.LG | 2022-07 Change to browse by: References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation BibTeX formatted citation Data provided by: Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Institution About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) Privacy Policy Web Accessibility Assistance arXiv Operational Status
